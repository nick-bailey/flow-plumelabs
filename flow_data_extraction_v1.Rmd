title: "Flow - data extraction - v1"
author: "Nick Bailey"
date: "30/03/2021"
output: html_document
---

TO DO:
* REFINE THE LINK OF MEASURES TO SPATIAL POINTS - HALF MINUTES?
* PROBLEM IN PENULTIMATE SECTION OF df_meas_orig2 HAVING MORE CASES THAN PREVIOUS
* - NEED TO DE-DUPLICATE df_sample?


# Analysis of data  from Flow 2 by Plume Labs

Flow 2 from Plume labs is a personal air quality sensor. Users can view data through a companion app but they can also download it for further analysis. This note shows an example of how the data can be imported and analysed. 

This file opens my full dataset, extracts points within the sample area/time and saves them to a new zip file in the directory 'data' which can then be shared. 

## Setup
Within the Flow app, users can go to 'Settings' and 'Export my data' to have a link emailed to them which enables them to download all of the data they have generated to date. The data arrive in a .zip file which is saved in the 'data-local-only' folder within this project. Older .zip files do not need to be removed - the code looks for the most recent and uses that. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# load packages
pacman::p_load(sf, vroom, lubridate, here, osmdata, ggmap, tidyverse)

# # old
# pacman::p_load(sf, vroom, lubridate, here, OpenStreetMap, osmdata, read_osm, tidyverse)

# lots of issues with OSM packages
# - OpenStreetMap repeatedly failed to load
# - read_osm - not available for this version of R

```




At the start, we read in boundaries for study area and create lists.

```{r locations, include=FALSE}

# read in boundaries for study area
# - NB that the names did not import with these
sf_sample_area_kml <- read_sf(here("geodata", "sample_area.kml")) %>% 
  mutate(Name = c("sample_area", "crow_road", "elmwood_lane", 
                  "elmwood_randolph", "woodcroft_edgehill"), 
         Name = factor(Name, 
                       levels = c("sample_area", "crow_road", "elmwood_lane", 
                                  "elmwood_randolph", "woodcroft_edgehill")))

# plot all boundaries
plot(sf_sample_area_kml[1])

# # plot sample area boundary
# plot(sf_sample_area_kml[1,])


```


## Get data zip file structure and contents
We identify the most recent .zip file in the 'data-local-only' folder, then get the structure and file names.

The .zip file contains:

* 'measures' (csv) - air quality readings with date/time
* 'positions' (csv) - lat/long with date/time
* 'positions' (kml)
* images directory with png file(s)

```{r files}

# zip file - name of most recent .zip in 'data' directory
# - doesn't work in one go for some reason
zip_file <- data.frame(names = list.files(here("data-local-only"))) 
zip_file <- zip_file %>%
  cbind(file.info(here("data-local-only", zip_file$names))) %>%
  filter(grepl("zip", names)) %>%
  filter(mtime == max(mtime)) %>%
  pull(names)

# zip file structure
zip_file_structure <- unzip(here("data-local-only", zip_file), list = TRUE) 

# file name - 'measures' csv
# - NB returns more than one if relevant
file_measures <- zip_file_structure %>%
  filter(grepl("measures", Name)) %>%
  pull(Name)

# file name - 'positions' csv
file_positions_csv <- zip_file_structure %>%
  filter(grepl("positions", Name)) %>%
  filter(grepl("csv", Name)) %>%
  pull(Name)

# file name - 'positions' kml
file_positions_kml <- zip_file_structure %>%
  filter(grepl("positions", Name)) %>%
  filter(grepl("kml", Name)) %>%
  pull(Name)


```

## Read in measures and positions data

The 'measures' file has timestamp and datetime, and measures for each of the five air quality items, recorded in original units and on Plume's standardised scale. 


```{r csv measures}

# read 'measures' csv 
# - NB reads multiple files if relevant
df_meas_orig <- vroom(map(file_measures, ~ unz(here("data-local-only", zip_file), .x))) 

# tidy colnames
# datetime rounded to nearest whole minute - for joining
df_meas <- df_meas_orig %>%
  rename(datetime_meas = "date (UTC)") %>%
  rename_with(~ sub(" (Plume AQI)", "_plume", .x, fixed = TRUE)) %>%
  rename_with(~ sub(" (ppb)", "", .x, fixed = TRUE)) %>%
  rename_with(~ sub(" (ug/m3)", "", .x, fixed = TRUE)) %>%
  rename_with(~ sub(" ", "_", .x, fixed = TRUE)) %>% 
  mutate(datetime_hm = round_date(datetime_meas, unit = "minute"))



```

The locations or positions are recorded when the Flow 2 is connected to the mobile phone via Bluetooth and the user has agreed to share location with Plume. They are recorded in the 'positions' file which has lat/long but also timestamp and datetime (in POSIXct format). We need these positions as the datetime are not present in the .kml file.

Also note that the timings for which position is recorded are not the same as the timings for which air pollution measurements are recorded so we need some approximation to join these up; see below.  


```{r positions csv}

# read 'positions' csv
df_pos_csv_orig <- vroom(map(file_positions_csv, ~ unz(here("data-local-only", zip_file), .x))) 

df_pos_csv <- df_pos_csv_orig %>% 
  rename(datetime_csv = date)

```

Positions are also recorded in the .kml file. Apart from the location information, it has two fields ('Name' and 'Description') both of which appear to be empty but no date/time field. We datetime from the 'positions' .csv file ('df_pos_csv'). There are the same number of records in each. 

```{r positions kml}

# read 'positions' kml
sf_pos_kml_orig <- read_sf(unzip(here("data-local-only", zip_file), files = file_positions_kml)) 

# tidy
# datetime rounded to nearest whole minute - for joining
sf_pos_kml <- sf_pos_kml_orig %>% 
  cbind(df_pos_csv$datetime_csv) %>% 
  rename(datetime_kml = df_pos_csv.datetime_csv) %>% 
  mutate(datetime_hm = round_date(datetime_kml, unit = "minute"))


# tidy up - delete folders+files created by unzip
unlink("flow", recursive = TRUE)


```



## Joining pollution measures to position
Easiest way to join is by rounding datetime in both position and measure databases to the nearest minute, and attaching the relevant measure record based on the same rounding. There will be multiple positions records with the same time in rounded minutes, each matched to the same measures record (since these are every minute). We therefore select the positions record which is nearest in timing to the relevant measures record (both measured to the second). 

Note that this isn't necessarily the closest record in time since the rounding process could put that in a different group (by rounded minutes). And a more sophisticated approach would be to attach the measures records for timings either side of position timing, and interpolate a measure for the specific timing of the position. But that seems more work than necessary at the moment. 

We can then make maps of pollution - here, PM10. 

``` {r join pos meas}

sf_pos_meas_kml <- sf_pos_kml %>% 
  left_join(df_meas, by = "datetime_hm") %>%
  mutate(datetime_diff = abs(datetime_kml - datetime_meas)) %>%    #abs value of diff
  group_by(datetime_hm) %>% 
  filter(datetime_diff == min(datetime_diff))


# XXXXXXXXXX TEMP LEFT HERE
# 
# # narrow data to Feb-Apr 2021, weekdays, 08.30-09.00
# df_meas <- df_meas %>% 
#   filter(date(datetime) >= "2021-02-01" &
#            date(datetime) <= "2021-04-30" & 
#            wday(datetime, week_start = 1) <= 5 &
#            hour(datetime) == 8 & minute(datetime) >= 30)
# 
# # narrow data to Feb-Apr 2021, weekdays, 08.30-09.00
# df_pos_csv <- df_pos_csv %>% 
#   filter(date(datetime) >= "2021-02-01" &
#            date(datetime) <= "2021-04-30" & 
#            wday(datetime, week_start = 1) <= 5 &
#            hour(datetime) == 8 & minute(datetime) >= 30)
# 
# # narrow data to Feb-Apr 2021, weekdays, 08.30-09.00
# sf_pos_kml <- sf_pos_kml %>% 
#   filter(date(datetime) >= "2021-02-01" &
#            date(datetime) <= "2021-04-30" & 
#            wday(datetime, week_start = 1) <= 5 &
#            hour(datetime) == 8 & minute(datetime) >= 30)


```

## Points in sample area

Identify points within sample area. 

``` {r sample}

# list of points in sample buffer & within relevant dates/times
# - NB using the 1st polygon from sf_sample_area_kml
df_sample <- as.data.frame(st_within(sf_pos_meas_kml, sf_sample_area_kml[1,], sparse = FALSE)) %>%
  rename(yn_area = V1) %>% 
  mutate(datetime_sample = sf_pos_meas_kml$datetime_kml) %>% 
  mutate(yn_datetime = case_when(date(datetime_sample) >= "2021-02-01" &
                                   date(datetime_sample) <= "2021-04-30" &
                                   wday(datetime_sample, week_start = 1) <= 5 &
                                   hour(datetime_sample) == 8 & 
                                   minute(datetime_sample) >= 30 ~ 1,
                                 TRUE ~ 0),
         yn = yn_area * yn_datetime)

# map of points in sample area & correct datetime
sf_pos_meas_kml %>% 
  cbind(df_sample) %>%
  filter(yn == 1) %>% 
  ggplot() +
  geom_sf(aes(colour = pm_10), size = 3) +
  scale_colour_continuous(type = "viridis", direction = -1) +
  coord_sf(xlim = c(-4.325, -4.318), ylim = c(55.877, 55.881), expand = FALSE)

# df with timestamps for points in study area
df_sample_datetime <- df_sample_area %>% 
  mutate(datetime_hm = sf_pos_meas_kml$datetime_hm) %>% 
  filter(yn == 1)


```

## Cut down original data to sample area/time

``` {r sample}

df_sample2 <- df_sample %>% 
  select(datetime_sample, yn) %>% 
  mutate(datetime_sample = round_date(datetime_sample, unit = "minute")) %>% 
  filter(yn == 1)

x <- df_sample2 %>% 
  select(date)

x <- unique(df_sample2$datetime_sample)
length(x)

df_meas_orig2 <- df_meas_orig %>% 
  mutate(datetime_sample = round_date(`date (UTC)`, unit = "minute")) %>% 
  left_join(df_sample2, by = "datetime_sample") %>% 
  filter(yn == 1) %>% 
  select(-c(datetime_sample, yn))

df_pos_csv_orig2 <- df_pos_csv_orig %>% 
  mutate(datetime_sample = round_date(date, unit = "minute")) %>% 
  left_join(df_sample2, by = "datetime_sample") 

df_pos_kml_orig2 <- df_pos_kml_orig %>% 
  cbind(df_pos_csv_orig2$yn)

# %>% 
#   filter(yn == 1) %>% 
#   select(-c(datetime_sample, yn))



```


## Zip the files

``` {r sample}

# make directory 'flow'
dir.create("flow")

# write files there
write.csv(df_meas_orig2, here("flow", "measures.csv"))

# create zip file in 'data' folder
zip(zipfile = here("data", "sample.zip"), 
    files = "flow")

# remove directory 'flow'
unlink("flow", recursive = TRUE)


```